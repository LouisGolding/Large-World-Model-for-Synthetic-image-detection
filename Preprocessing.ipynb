{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install pillow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All zip files have been extracted!\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import zipfile\n",
    "\n",
    "root_folder = './data'\n",
    "\n",
    "def unzip_all_in_folder(folder_path):\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.zip'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Create a new folder with the same name as the zip file (without the .zip extension)\n",
    "                extract_folder = os.path.join(root, os.path.splitext(file)[0])\n",
    "                os.makedirs(extract_folder, exist_ok=True)\n",
    "                \n",
    "                with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(extract_folder)\n",
    "                print(f'Unzipped: {file_path} to {extract_folder}')\n",
    "                os.remove(file_path)  # Delete the zip file after extracting its contents\n",
    "\n",
    "unzip_all_in_folder(os.path.join(root_folder, 'real'))\n",
    "unzip_all_in_folder(os.path.join(root_folder, 'fake'))\n",
    "\n",
    "print(\"All zip files have been extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced train and test dataset creation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "# Directories\n",
    "real_dir = 'data/real'\n",
    "fake_dir = 'data/fake'\n",
    "output_dir = 'sets'\n",
    "train_dir = os.path.join(output_dir, 'train')\n",
    "test_dir = os.path.join(output_dir, 'test')\n",
    "\n",
    "# Create the output directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Remove resizing and directly save images\n",
    "def save_image(image_path, output_path, quality=90):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img.save(output_path, 'JPEG', quality=quality)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "# Function to split images into train and test\n",
    "def process_images(image_paths, num_test, num_train, dataset_type, category, output_test, output_train):\n",
    "    random.shuffle(image_paths)  # Shuffle images to ensure random sampling\n",
    "    \n",
    "    test_images = image_paths[:num_test]  # Select num_test images for testing\n",
    "    train_images = image_paths[num_test:num_test + num_train]  # Select num_train images for training\n",
    "    \n",
    "    # Copy images to test and train directories\n",
    "    for img in test_images:\n",
    "        output_path = os.path.join(output_test, f'{dataset_type}_{category}_{os.path.basename(img)}')\n",
    "        save_image(img, output_path, quality=random.randint(50, 100))\n",
    "    \n",
    "    for img in train_images:\n",
    "        output_path = os.path.join(output_train, f'{dataset_type}_{category}_{os.path.basename(img)}')\n",
    "        save_image(img, output_path, quality=random.randint(50, 100))\n",
    "\n",
    "# Recursively get all images in a directory\n",
    "def get_images_in_directory(dir_path):\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            if file.endswith(('png', 'jpg', 'jpeg')):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return image_paths\n",
    "\n",
    "# Real and Fake categories setup\n",
    "real_categories = ['coco', 'ffhq', 'imagenet', 'lsun']\n",
    "fake_categories = ['generative_inpainting', 'glide', 'stylegan2', 'stylegan3', 'taming_transformer']\n",
    "\n",
    "# Total number of images needed for real and fake categories\n",
    "num_test_real = 625  # Test images per real category\n",
    "num_train_real = 5625  # Train images per real category (9x the test set)\n",
    "num_test_fake = 500  # Test images per fake category\n",
    "num_train_fake = 4500  # Train images per fake category (9x the test set)\n",
    "\n",
    "# Process real images (balanced between 4 categories)\n",
    "for category in real_categories:\n",
    "    real_images = get_images_in_directory(os.path.join(real_dir, category))\n",
    "    if len(real_images) < num_test_real + num_train_real:\n",
    "        print(f\"Not enough images in category {category}. Needed: {num_test_real + num_train_real}, found: {len(real_images)}\")\n",
    "        continue  # Skip categories that don't have enough images\n",
    "    process_images(real_images, num_test_real, num_train_real, 'real', category, test_dir, train_dir)\n",
    "\n",
    "# Process fake images (balanced between 5 categories)\n",
    "for category in fake_categories:\n",
    "    fake_images = get_images_in_directory(os.path.join(fake_dir, category))\n",
    "    if len(fake_images) < num_test_fake + num_train_fake:\n",
    "        print(f\"Not enough images in category {category}. Needed: {num_test_fake + num_train_fake}, found: {len(fake_images)}\")\n",
    "        continue  # Skip categories that don't have enough images\n",
    "    process_images(fake_images, num_test_fake, num_train_fake, 'fake', category, test_dir, train_dir)\n",
    "\n",
    "# Generate dataset structure for Hugging Face or JSON\n",
    "def create_dataset_metadata(image_dir):\n",
    "    dataset = []\n",
    "    for root, _, files in os.walk(image_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(('png', 'jpg', 'jpeg')):\n",
    "                image_path = os.path.join(root, file)\n",
    "                # Check if the file path contains 'real' or 'fake' in the name and assign the correct label\n",
    "                if 'real' in file:\n",
    "                    label = 'real'\n",
    "                elif 'fake' in file:\n",
    "                    label = 'fake'\n",
    "                else:\n",
    "                    label = 'unknown'\n",
    "                data = {\n",
    "                    'image_path': image_path,\n",
    "                    'label': label\n",
    "                }\n",
    "                dataset.append(data)\n",
    "    return dataset\n",
    "\n",
    "# Create metadata for Hugging Face dataset structure\n",
    "train_data = create_dataset_metadata(train_dir)\n",
    "test_data = create_dataset_metadata(test_dir)\n",
    "\n",
    "with open(os.path.join(output_dir, 'train_dataset.json'), 'w') as f:\n",
    "    json.dump(train_data, f, indent=4)\n",
    "\n",
    "with open(os.path.join(output_dir, 'test_dataset.json'), 'w') as f:\n",
    "    json.dump(test_data, f, indent=1)\n",
    "\n",
    "\n",
    "print(\"Balanced train and test dataset creation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for Train dataset:\n",
      "  Real images: 22445\n",
      "    lsun: 5570\n",
      "    ffhq: 5625\n",
      "    imagenet: 5625\n",
      "    coco: 5625\n",
      "  Fake images: 20614\n",
      "    glide: 3910\n",
      "    stylegan3: 3488\n",
      "    stylegan2: 4408\n",
      "    taming: 4392\n",
      "    generative: 4416\n",
      "\n",
      "Summary for Test dataset:\n",
      "  Real images: 2498\n",
      "    lsun: 623\n",
      "    imagenet: 625\n",
      "    ffhq: 625\n",
      "    coco: 625\n",
      "  Fake images: 2469\n",
      "    glide: 491\n",
      "    stylegan3: 485\n",
      "    taming: 499\n",
      "    generative: 498\n",
      "    stylegan2: 496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Directories for train and test sets\n",
    "train_dir = 'sets/train'\n",
    "test_dir = 'sets/test'\n",
    "\n",
    "# Function to summarize the dataset\n",
    "def summarize_images(image_dir):\n",
    "    summary = defaultdict(lambda: defaultdict(int))  # Nested defaultdict for category and type (real/fake)\n",
    "    \n",
    "    # Walk through each image in the directory and categorize it\n",
    "    for root, _, files in os.walk(image_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(('png', 'jpg', 'jpeg')):\n",
    "                file_name = file.lower()\n",
    "                if 'real' in file_name:\n",
    "                    dataset_type = 'real'\n",
    "                elif 'fake' in file_name:\n",
    "                    dataset_type = 'fake'\n",
    "                else:\n",
    "                    dataset_type = 'unknown'  # If file is not classified as real or fake (just in case)\n",
    "                \n",
    "                # Extract category from the file name\n",
    "                category = file_name.split('_')[1]  # Assumes file names are structured as: type_category_filename\n",
    "\n",
    "                # Update count for the specific dataset type and category\n",
    "                summary[dataset_type][category] += 1\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Function to print the summary\n",
    "def print_summary(summary, dataset_name):\n",
    "    print(f\"Summary for {dataset_name} dataset:\")\n",
    "    for dataset_type, categories in summary.items():\n",
    "        total_images = sum(categories.values())\n",
    "        print(f\"  {dataset_type.capitalize()} images: {total_images}\")\n",
    "        for category, count in categories.items():\n",
    "            print(f\"    {category}: {count}\")\n",
    "    print()\n",
    "\n",
    "# Summarize train and test datasets\n",
    "train_summary = summarize_images(\"sets/train\")\n",
    "test_summary = summarize_images(\"sets/test\")\n",
    "\n",
    "# Print out the summaries\n",
    "print_summary(train_summary, \"Train\")\n",
    "print_summary(test_summary, \"Test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "world",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
